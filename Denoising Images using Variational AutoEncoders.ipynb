{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "453a9f00-67c7-4d4a-abe8-cd2d83654f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4575a81d-8dbf-4261-a7ad-24b0958f4642",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 784\n",
    "hidden_dim = 400\n",
    "latent_dim = 20\n",
    "batch_size = 128\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6ee3d65-eee4-49a4-a919-d40ff2a68df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18d3d884-a1e6-4ea3-8140-5ce62844ca66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.FashionMNIST(root=\"./data\",\n",
    "                                                  train = True,\n",
    "                                                  transform = transforms.ToTensor(),\n",
    "                                                  download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle = True)\n",
    "\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root=\"./data\",\n",
    "                                                  train = False,\n",
    "                                                  transform = transforms.ToTensor(),\n",
    "                                                  download=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle = False)\n",
    "\n",
    "# save img to dir\n",
    "sample_dir = \"imgs\"\n",
    "\n",
    "if not os.path.exists(sample_dir):\n",
    "    os.makedirs(sample_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "128e4472-0235-45a7-9be3-9ce23a6e713b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE,self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(img_size,hidden_dim)\n",
    "        self.fc2_mean = nn.Linear(hidden_dim,latent_dim)\n",
    "        self.fc2_logvar = nn.Linear(hidden_dim,latent_dim)\n",
    "        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, img_size)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.F.relu(self.fc1(x))\n",
    "        mu = self.fc2_mean(h)\n",
    "        logvar = self.fc2_logvar(h)\n",
    "\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu,logvar):\n",
    "        std = torch.exp(logvar/2)\n",
    "        eps = torch.randn_like(std)\n",
    "        \n",
    "        return mu + eps*std\n",
    "        \n",
    "    def decode(self,z):\n",
    "        h = F.relu(self.fc3(z))\n",
    "        out = torch.sigmoid(self.fc4(h))\n",
    "\n",
    "        return out\n",
    "\n",
    "    def forward(x):\n",
    "        mu,logvar = self.encode(x.view(-1,img_size))\n",
    "        z = self.reparameterize(mu,logv)\n",
    "        out = self.decode(z)\n",
    "\n",
    "        return out,mu,logvar\n",
    "\n",
    "\n",
    "model = VAE().to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40fe39ad-0064-4dd6-b6c2-f41ff87c2eda",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (378109805.py, line 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[13], line 24\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(f\"Epoch {epoch}, Average Loss: {(train_loss/len(train_loader.dataset)):.3f}\")\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def loss_fn(out,img,mu,logvar):\n",
    "    bce = F.binary_cross_entropy(out,img.view(-1,img_size), reduction=\"sum\")\n",
    "    kld = 0.5*torch.sum(logvar.exp() + mu**2 - 1 - logvar)\n",
    "\n",
    "    return bce+kld\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for i,(images,_) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        out,mu,logvar = model(images)\n",
    "        loss = loss_fn(out,images,mu,logvar)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        train_loss+=loss.item()\n",
    "\n",
    "        if i%100==0:\n",
    "            print(\"Train Epoch {} [Batch {}/{}]\\tLoss: {:3f}\".format(epoch, i, len(train_loader), loss.item()/len(images))\n",
    "    \n",
    "    print(f\"Epoch {epoch}, Average Loss: {(train_loss/len(train_loader.dataset)):.3f}\")\n",
    "                  \n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx,(images,_) in enumerate(test_loader):\n",
    "            images = images.to(device)\n",
    "            out,mu,logvar = model(images)\n",
    "            test_loss += loss_fn(out,images,mu,logvar).item()\n",
    "\n",
    "            if batch_idx == 0:\n",
    "                comparison = torch.cat([images[:5], out.view(batch_size,1,28,28)[:5]])\n",
    "                save_image(comparison.cpu(), 'results/out_' + str(epoch) + '.png',nrow=5)\n",
    "                \n",
    "        print(\">> Average Loss: {:..3f}\".format(test_loss/len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9a0bb2-60b2-41a6-b7e1-5079bf5baba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aef227b-89d0-4666-977c-170334d37941",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1,epoch+1):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "\n",
    "    with torch.no_grad:\n",
    "        sample = torch.randn(64,20).to(device)\n",
    "        generated = model.decode(sample).cpu()\n",
    "        save_img(generated.view(64,1,28,28), 'results/sample_' + str(epoch)+'.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
